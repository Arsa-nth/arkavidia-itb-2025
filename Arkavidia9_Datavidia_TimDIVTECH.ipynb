{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from itertools import product\n",
    "from scipy.stats.mstats import winsorize\n",
    "from joblib import Parallel, delayed, dump, load  # Import joblib, dump, load\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import traceback #import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpmdarima\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpm\u001b[39;00m  \u001b[38;5;66;03m# Import pmdarima\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pmdarima\\__init__.py:52\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __check_build\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Stuff we want at top-level\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marima\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m auto_arima, ARIMA, AutoARIMA, StepwiseContext, decompose\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m acf, autocorr_plot, c, pacf, plot_acf, plot_pacf, \\\n\u001b[32m     54\u001b[39m     tsdisplay\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pmdarima\\arima\\__init__.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapprox\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marima\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pmdarima\\arima\\approx.py:9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# R approx function\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m c, check_endog\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_callable\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DTYPE\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pmdarima\\utils\\__init__.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetaestimators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvisualization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pmdarima\\utils\\array.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DTYPE\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m C_intgrt_vec\n\u001b[32m     15\u001b[39m __all__ = [\n\u001b[32m     16\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mas_series\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     17\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mc\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mis_iterable\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     23\u001b[39m ]\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mas_series\u001b[39m(x, **kwargs):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpmdarima\\\\utils\\\\_array.pyx:1\u001b[39m, in \u001b[36minit pmdarima.utils._array\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import pmdarima as pm  # Import pmdarima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Functions ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Data Harga Pangan (Train, Test, Sample Submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_price_data(folder_path):\n",
    "    all_data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            temp_df = pd.read_csv(file_path, thousands=',', decimal='.')\n",
    "            temp_df['Date'] = pd.to_datetime(temp_df['Date'])\n",
    "            temp_df['item'] = filename.replace(\".csv\",\"\").replace(\" \", \"_\")\n",
    "            all_data.append(temp_df)\n",
    "    return pd.concat(all_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Data Google Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_google_trends(main_folder):\n",
    "    all_trends = {}\n",
    "    for item_folder in os.listdir(main_folder):\n",
    "        item_path = os.path.join(main_folder, item_folder)\n",
    "        if os.path.isdir(item_path):\n",
    "            item_name = item_folder.replace(\" \", \"_\")\n",
    "            all_trends[item_name] = []\n",
    "            for province_file in os.listdir(item_path):\n",
    "                if province_file.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(item_path, province_file)\n",
    "                    trend_df = pd.read_csv(file_path, thousands=',', decimal='.')\n",
    "                    if 'Date' in trend_df.columns:\n",
    "                        trend_df['date'] = pd.to_datetime(trend_df['Date'])\n",
    "                        trend_df.drop(columns=['Date'], inplace=True)\n",
    "                    else:\n",
    "                        print(f\"Warning: File {file_path} missing 'Date' column.\")\n",
    "                        continue\n",
    "                    trend_df.rename(columns={trend_df.columns[0]: item_name}, inplace=True)\n",
    "                    province_name = province_file.replace(\".csv\", \"\").replace(\" \", \"_\")\n",
    "                    trend_df['provinsi'] = province_name\n",
    "                    all_trends[item_name].append(trend_df)\n",
    "    return all_trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load Data Komoditas Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_commodity_data(folder_path):\n",
    "   all_commodities = {}\n",
    "   for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            commodity_df = pd.read_csv(file_path, thousands=',', decimal='.')\n",
    "            commodity_name = filename.replace(\" Historical Data.csv\", \"\").replace(\" \", \"_\").replace(\"#\", \"num\")\n",
    "            commodity_df['Date'] = pd.to_datetime(commodity_df['Date'])\n",
    "            commodity_df = commodity_df.rename(columns={'Date': 'date'})\n",
    "            all_commodities[commodity_name] = commodity_df\n",
    "   return all_commodities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load Data Mata Uang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_currency_data(folder_path):\n",
    "  all_currencies = {}\n",
    "  for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            curr_df = pd.read_csv(file_path, thousands=',', decimal='.')\n",
    "            currency_code = filename.replace(\"=X.csv\", \"\")\n",
    "            curr_df['Date'] = pd.to_datetime(curr_df['Date'])\n",
    "            curr_df = curr_df.rename(columns={'Date': 'date', 'Close': currency_code})\n",
    "            curr_df = curr_df[['date',currency_code]]\n",
    "            all_currencies[currency_code] = curr_df\n",
    "\n",
    "  return all_currencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_main_data(df):\n",
    "    df[['item', 'provinsi', 'date']] = df['id'].str.split('/', expand=True)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(df):\n",
    "    df = df.melt(id_vars=['Date', 'item'], var_name='provinsi', value_name='price')\n",
    "    df = df.rename(columns={'Date':'date'})\n",
    "    df['id'] = df['item'] + '/' + df['provinsi'] + '/' + df['date'].dt.strftime('%Y-%m-%d')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. FE Data Utama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_features(df):\n",
    "    df['year'] = df['date'].dt.year.astype(int)\n",
    "    df['month'] = df['date'].dt.month.astype(int)\n",
    "    df['day'] = df['date'].dt.day.astype(int)\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek.astype(int)\n",
    "    df['dayofyear'] = df['date'].dt.dayofyear.astype(int)\n",
    "    df['weekofyear'] = df['date'].dt.isocalendar().week.astype(int)\n",
    "    df['quarter'] = df['date'].dt.quarter.astype(int)\n",
    "    df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "    df['is_quarter_start'] = df['date'].dt.is_quarter_start.astype(int)\n",
    "    df['is_quarter_end'] = df['date'].dt.is_quarter_end.astype(int)\n",
    "    df['is_year_start'] = df['date'].dt.is_year_start.astype(int)\n",
    "    df['is_year_end'] = df['date'].dt.is_year_end.astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_agg_features(df, lags, windows, group_cols, target_col):\n",
    "    for lag in lags:\n",
    "        df[f'{target_col}_lag_{lag}'] = df.groupby(group_cols)[target_col].shift(lag)\n",
    "        df[f'{target_col}_lag_{lag}_diff'] = df.groupby(group_cols)[target_col].diff(lag)\n",
    "    for window in windows:\n",
    "        df[f'{target_col}_mean_{window}'] = df.groupby(group_cols)[target_col].transform(lambda x: x.rolling(window=window).mean())\n",
    "        df[f'{target_col}_std_{window}'] = df.groupby(group_cols)[target_col].transform(lambda x: x.rolling(window=window).std())\n",
    "        df[f'{target_col}_mean_{window}_diff'] = df.groupby(group_cols)[f'{target_col}_lag_1_diff'].transform(lambda x: x.rolling(window=window).mean())\n",
    "        df[f'{target_col}_std_{window}_diff'] = df.groupby(group_cols)[f'{target_col}_lag_1_diff'].transform(lambda x: x.rolling(window=window).std())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. FE Google Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_google_trends(df):\n",
    "    df = df.copy()\n",
    "    # Feature Engineering: Lag Features (Tambahkan lag yang lebih panjang)\n",
    "    df = create_lag_agg_features(df, lags=[1, 7, 30, 90, 180, 365], windows=[3, 7, 14], group_cols=['provinsi'], target_col=df.columns[1])\n",
    "\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['dayofyear'] = df['date'].dt.dayofyear\n",
    "    df['weekofyear'] = df['date'].dt.isocalendar().week.astype(int)\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "    df['is_quarter_start'] = df['date'].dt.is_quarter_start.astype(int)\n",
    "    df['is_quarter_end'] = df['date'].dt.is_quarter_end.astype(int)\n",
    "    df['is_year_start'] = df['date'].dt.is_year_start.astype(int)\n",
    "    df['is_year_end'] = df['date'].dt.is_year_end.astype(int)\n",
    "\n",
    "    holidays = ['2022-01-01', '2022-02-01', '2022-02-28', '2022-03-03', '2022-04-15', '2022-04-29',  '2022-05-01', '2022-05-02','2022-05-03',\n",
    "            '2022-05-04','2022-05-05','2022-05-06','2022-05-16', '2022-05-26', '2022-06-01', '2022-07-09', '2022-07-30', '2022-08-17','2022-10-08', '2022-12-25',\n",
    "            '2023-01-01', '2023-01-22','2023-01-23', '2023-02-18', '2023-03-22', '2023-03-23', '2023-04-07', '2023-04-19','2023-04-20','2023-04-21',\n",
    "            '2023-04-22', '2023-04-23','2023-04-24', '2023-04-25','2023-05-01','2023-05-18', '2023-06-01','2023-06-02','2023-06-04',\n",
    "            '2023-06-28','2023-06-29','2023-06-30', '2023-07-19', '2023-08-17','2023-09-28', '2023-12-25','2023-12-26',\n",
    "            '2024-01-01', '2024-02-08', '2024-02-09', '2024-02-10', '2024-03-11','2024-03-12', '2024-03-29','2024-03-31', '2024-04-08', '2024-04-09', '2024-04-10',\n",
    "            '2024-04-11','2024-04-12','2024-04-15', '2024-05-01', '2024-05-09', '2024-05-10', '2024-05-23', '2024-05-24', '2024-06-01', '2024-06-17', '2024-06-18',\n",
    "            '2024-07-07','2024-08-17','2024-09-16']\n",
    "    df['is_holiday'] = df['date'].isin(holidays).astype(int)\n",
    "\n",
    "    columns_to_impute = [col for col in df.columns if 'lag' in col or 'rolling' in col]\n",
    "    for col in columns_to_impute:\n",
    "        if df[col].isnull().any():\n",
    "            df[col] = df.groupby('provinsi')[col].transform(lambda x: x.fillna(x.median()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. FE Data Mata Uang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe_currency_data(currency_data):\n",
    "    processed_currency = {}\n",
    "    for currency, curr_df in currency_data.items():\n",
    "        curr_df[f'{currency}_lag1'] = curr_df[currency].shift(1)\n",
    "        curr_df[f'{currency}_mean7'] = curr_df[currency].rolling(window=7).mean()\n",
    "        curr_df[currency] = winsorize(curr_df[currency], limits=[0.05, 0.05])\n",
    "        processed_currency[currency] = curr_df\n",
    "    return processed_currency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. FE Data Komoditas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe_commodity_data(commodity_data):\n",
    "    processed_commodity = {}\n",
    "    for commodity_name, commodity_df in commodity_data.items():\n",
    "        commodity_df.drop(columns=['Open', 'High', 'Low', 'Vol.', 'Change %'], inplace=True, errors='ignore')\n",
    "        commodity_df = commodity_df.rename(columns={'Price': commodity_name})\n",
    "        commodity_df[commodity_name] = winsorize(commodity_df[commodity_name], limits=[0.05, 0.05])\n",
    "        commodity_df[f'{commodity_name}_lag1'] = commodity_df[commodity_name].shift(1)\n",
    "        commodity_df[f'{commodity_name}_mean7'] = commodity_df[commodity_name].rolling(window=7).mean()\n",
    "        commodity_df[f'{commodity_name}_std7'] = commodity_df[commodity_name].rolling(window=7).std()\n",
    "        processed_commodity[commodity_name] = commodity_df\n",
    "    return processed_commodity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Function Lainya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactions(df, all_trends):\n",
    "  df = df.copy()\n",
    "  for item_name in all_trends.keys():\n",
    "      if item_name in df.columns:\n",
    "          df[f'{item_name}_x_USDIDR'] = df[item_name] * df['currency_USDIDR']\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_data(main_df, all_trends, currency_data, commodity_data):\n",
    "    merged_df = main_df.copy()\n",
    "\n",
    "    for item_name, trend_df in all_trends.items():\n",
    "        merged_df = pd.merge(merged_df, trend_df, how='left', on=['date', 'provinsi'])\n",
    "\n",
    "    for currency, curr_df in currency_data.items():\n",
    "        if 'date' in curr_df.columns:\n",
    "            merged_df = pd.merge(merged_df, curr_df, on='date', how='left')\n",
    "        else:\n",
    "            print(f\"Warning: Kolom 'date' tidak ditemukan di dataframe untuk {currency}.\")\n",
    "\n",
    "    for commodity_name, commodity_df in commodity_data.items():\n",
    "        if 'date' in commodity_df.columns:\n",
    "             merged_df = pd.merge(merged_df, commodity_df, on='date', how='left')\n",
    "        else:\n",
    "            print(f\"Warning: Kolom 'date' tidak ditemukan di dataframe untuk {commodity_name}.\")\n",
    "\n",
    "    # Imputasi sebelum SimpleImputer\n",
    "    for col in merged_df.columns:\n",
    "        if merged_df[col].isnull().any():\n",
    "            if col in all_trends.keys():\n",
    "              merged_df[col] = merged_df[col].fillna(0)\n",
    "            elif 'currency' in col or 'lag' in col or 'mean' in col or 'std' in col:\n",
    "              merged_df[col] = merged_df.groupby(['item', 'provinsi'])[col].ffill()\n",
    "            elif col in commodity_data.keys():\n",
    "              merged_df[col] = merged_df[col].ffill()\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(series, d=1):\n",
    "    \"\"\"Applies differencing to a Pandas Series.\"\"\"\n",
    "    return series.diff(periods=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_difference(first_value, forecast):\n",
    "    \"\"\"Inverse differencing for a single forecast value.  Removed 'series'.\"\"\"\n",
    "    return first_value + forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Load Data ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = r'c:\\Users\\mikae\\OneDrive\\Documents\\Lomba\\DataVidia\\Penyisihan\\Harga Bahan Pangan\\train'\n",
    "train_data = load_and_process_price_data(train_folder)\n",
    "train_df = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = r'c:\\Users\\mikae\\OneDrive\\Documents\\Lomba\\DataVidia\\Penyisihan\\Harga Bahan Pangan\\test'\n",
    "test_data = load_and_process_price_data(test_folder)\n",
    "test_df = test_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_trends_folder = r'C:\\Users\\mikae\\OneDrive\\Documents\\Lomba\\DataVidia\\Penyisihan\\Google Trends Merged' #corrected folder\n",
    "all_trends = load_google_trends(google_trends_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commodity_data = load_commodity_data(r'C:\\Users\\mikae\\OneDrive\\Documents\\Lomba\\DataVidia\\Penyisihan\\Global Commodity Price')\n",
    "currency_data = load_currency_data(r'C:\\Users\\mikae\\OneDrive\\Documents\\Lomba\\DataVidia\\Penyisihan\\Mata Uang')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Preprocessing & Feature Engineering ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = reshape_data(train_df)\n",
    "test_df = reshape_data(test_df)\n",
    "test_df = preprocess_main_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_date_features(train_df)\n",
    "test_df = create_date_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "label_encoders = {}\n",
    "for col in ['item', 'provinsi']:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    combined_data = pd.concat([train_df[col], test_df[col]], axis=0).astype(str)\n",
    "    label_encoders[col].fit(combined_data)\n",
    "    train_df[col] = label_encoders[col].transform(train_df[col].astype(str))\n",
    "    test_df[col] = label_encoders[col].transform(test_df[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Transform (before lag features)\n",
    "train_df['price'] = np.log1p(train_df['price'])\n",
    "#test_df TIDAK ditransformasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag and Agg Features (before merging)\n",
    "train_df = create_lag_agg_features(train_df, lags=[1, 2, 3, 7, 14, 30], windows=[3, 7, 14], group_cols=['item', 'provinsi'], target_col='price')\n",
    "test_df = create_lag_agg_features(test_df, lags=[1, 2, 3, 7, 14, 30], windows=[3, 7, 14], group_cols=['item', 'provinsi'], target_col='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Google Trends\n",
    "for item_name, trends_list in all_trends.items():\n",
    "    processed_list = []\n",
    "    for trend_df in trends_list:\n",
    "        processed_df = process_google_trends(trend_df)\n",
    "        processed_list.append(processed_df)\n",
    "    all_trends[item_name] = pd.concat(processed_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for Currency and Commodity Data\n",
    "currency_data = fe_currency_data(currency_data)\n",
    "commodity_data = fe_commodity_data(commodity_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Data\n",
    "train_df = merge_all_data(train_df, all_trends, currency_data, commodity_data)\n",
    "test_df = merge_all_data(test_df, all_trends, currency_data, commodity_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Interactions (after merging, before imputation)\n",
    "train_df = create_interactions(train_df, all_trends)\n",
    "test_df = create_interactions(test_df, all_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- SARIMA Modelling ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SARIMA Modeling (Simplified and Corrected) ---\n",
    "\n",
    "# 1. Prepare the data *BEFORE* the loop\n",
    "\n",
    "train_df_sarima = train_df.copy()\n",
    "\n",
    "# Set 'date' as index *temporarily*, for reindexing.\n",
    "train_df_sarima = train_df_sarima.set_index('date')\n",
    "\n",
    "# Create a complete date range for the ENTIRE training set\n",
    "full_date_range = pd.date_range(start=train_df_sarima.index.min(), end=train_df_sarima.index.max(), freq='D')\n",
    "\n",
    "\n",
    "# 2. Reindex, Fill, and Difference *BEFORE* the loop\n",
    "# Group first, *then* apply the reindexing WITHIN each group.\n",
    "# KEY CHANGE: group_keys=False\n",
    "train_df_sarima = (\n",
    "    train_df_sarima.groupby(['item', 'provinsi'], group_keys=False)  # Add group_keys=False\n",
    "    .apply(lambda x: x.reindex(full_date_range))\n",
    ")\n",
    "# 3. Forward-fill within each group (after reindexing!)\n",
    "train_df_sarima = train_df_sarima.ffill() #ffill directly\n",
    "\n",
    "# CRUCIAL FIX:  Bring 'date' back as a regular column *before* differencing.\n",
    "train_df_sarima = train_df_sarima.reset_index() #reset all index\n",
    "\n",
    "\n",
    "# 4. *Now* do differencing (after reindexing and filling)\n",
    "train_df_sarima['price_diff'] = train_df_sarima.groupby(['item', 'provinsi'])['price'].diff()\n",
    "\n",
    "\n",
    "# 5. drop NaN values that result from differencing (at beginning of each group)\n",
    "train_df_sarima.dropna(subset=['price_diff'], inplace=True)\n",
    "# train_df_sarima = train_df_sarima.reset_index() # Completely reset the index\n",
    "train_df_sarima = train_df_sarima.set_index(['item', 'provinsi', 'date']) # Now* set the multi-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameter Tuning Function ---\n",
    "def train_sarima(train_data, order, seasonal_order):\n",
    "    try:\n",
    "        model = SARIMAX(train_data, order=order, seasonal_order=seasonal_order,\n",
    "                       enforce_stationarity=False, enforce_invertibility=False)\n",
    "        model_fit = model.fit(disp=False) # Remove method\n",
    "        return model_fit\n",
    "    except Exception as e:\n",
    "        print(f\"SARIMA training failed with order={order}, seasonal_order={seasonal_order}. Error: {e}\") # Debugging\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parallel Loop with tqdm and Memory Mapping ---\n",
    "from tqdm import tqdm\n",
    "from joblib import dump, load  # Import for memory mapping\n",
    "\n",
    "# Create a memory-mapped array for train_df_sarima\n",
    "dump(train_df_sarima, 'train_df_sarima.joblib')\n",
    "train_df_sarima_memmap = load('train_df_sarima.joblib', mmap_mode='r')\n",
    "\n",
    "# --- PARALLEL PROCESSING ---\n",
    "def tune_sarima_for_group(group_data):  # Simplified arguments\n",
    "    item_id = group_data.index.get_level_values('item')[0]\n",
    "    provinsi_id = group_data.index.get_level_values('provinsi')[0]\n",
    "    #print(f\"Tuning for item={item_id}, provinsi={provinsi_id}\")\n",
    "\n",
    "    best_rmse = float('inf')\n",
    "    best_order = None\n",
    "    best_seasonal_order = None\n",
    "    results = []\n",
    "\n",
    "    train_subset = group_data['price_diff']  # Already reindexed, filled, and differenced\n",
    "    # print(train_subset) #debugging\n",
    "\n",
    "    if train_subset.empty or len(train_subset) < 10:  # Check for enough data.\n",
    "      #print(f\"Skipping item={item_id}, provinsi={provinsi_id} due to insufficient data.\")\n",
    "      return None, (item_id, provinsi_id), None  # Return None if not enough data\n",
    "\n",
    "    if group_data.index.names[2] is not None: #check date index name\n",
    "        if group_data.index.get_level_values('date').inferred_freq != 'D': #check date inferred_freq\n",
    "          print(f\"Warning: Could not set daily frequency for item={item_id}, provinsi={provinsi_id}\") # Debugging\n",
    "          return None, (item_id, provinsi_id), None #skip\n",
    "\n",
    "    # Parameter Grid - keep it reasonable\n",
    "    p = [0, 1, 2]  # Example: 0, 1, 2  -- REDUCED\n",
    "    d = [1]          # Fix d=1\n",
    "    q = [0, 1, 2]  # 0, 1, 2  -- REDUCED\n",
    "    P = [0, 1]  # 0, 1     -- REDUCED\n",
    "    D = [1]          # Fix D=1\n",
    "    Q = [0, 1]  # 0, 1     -- REDUCED\n",
    "    s = 7            # Weekly seasonality\n",
    "    pdq = list(product(p, d, q))\n",
    "    seasonal_pdq = [(x[0], x[1], x[2], s) for x in list(product(P, D, Q))]\n",
    "\n",
    "\n",
    "    for order in pdq:\n",
    "        for seasonal_order in seasonal_pdq:\n",
    "            model_fit = train_sarima(train_subset, order, seasonal_order)\n",
    "            if model_fit is None:\n",
    "                continue\n",
    "\n",
    "            # Use SARIMAXResults.predict (much faster)\n",
    "            predictions_diff = model_fit.predict()\n",
    "            #print(predictions_diff) #debugging\n",
    "\n",
    "            # Inverse difference.  MUCH SIMPLER NOW.\n",
    "            original_prices = group_data['price'].dropna() #keep index\n",
    "\n",
    "            # Use .values to get a NumPy array, and slicing, and addition:\n",
    "            predictions = original_prices.iloc[0] + np.cumsum(predictions_diff)\n",
    "\n",
    "            # Align for RMSE\n",
    "            original_prices_aligned = original_prices.iloc[1:]\n",
    "            predictions_aligned = predictions[:len(original_prices_aligned)]\n",
    "\n",
    "            #check length\n",
    "            if len(original_prices_aligned) != len(predictions_aligned):\n",
    "                # print(f\"Skipping due to length mismatch: original={len(original_prices_aligned)}, predicted={len(predictions_aligned)}\")  # Debug\n",
    "                continue  # Skip to the next parameter set.  VERY IMPORTANT.\n",
    "\n",
    "\n",
    "            # Check for NaNs *before* RMSE calculation\n",
    "            if np.isnan(original_prices_aligned).any() or np.isnan(predictions_aligned).any():\n",
    "                #print(f\"NaNs found in item={item_id}, prov={provinsi_id}, order={order}, sorder={seasonal_order}.  Skipping.\")\n",
    "                continue\n",
    "\n",
    "            rmse = np.sqrt(mean_squared_error(original_prices_aligned, predictions_aligned))\n",
    "            mape = mean_absolute_percentage_error(original_prices_aligned, predictions_aligned)\n",
    "            #print(f\"item={item_id}, provinsi={provinsi_id}, order={order}, seasonal_order={seasonal_order}, RMSE: {rmse}\") # Debugging output\n",
    "\n",
    "            results.append({\n",
    "                'item': item_id,\n",
    "                'provinsi': provinsi_id,\n",
    "                'order': order,\n",
    "                'seasonal_order': seasonal_order,\n",
    "                'rmse': rmse,\n",
    "                'mape': mape\n",
    "            })\n",
    "\n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_order = order\n",
    "                best_seasonal_order = seasonal_order\n",
    "                best_model = model_fit\n",
    "\n",
    "    return results, (item_id, provinsi_id), best_model\n",
    "\n",
    "# Wrap the loop with tqdm for a progress bar:\n",
    "with tqdm(total=len(list(train_df_sarima_memmap.groupby(['item', 'provinsi'])))) as pbar:\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(tune_sarima_for_group)(group_data)\n",
    "        for (item_id, provinsi_id), group_data in train_df_sarima_memmap.groupby(['item', 'provinsi'])\n",
    "    )\n",
    "    pbar.update(1)\n",
    "\n",
    "# --- Collect Results ---\n",
    "# Process results from parallel execution\n",
    "all_results = []\n",
    "best_models = {}\n",
    "for result_list, (item_id, provinsi_id), best_model in results:\n",
    "    if result_list is not None:  # Handle cases where a group might return None\n",
    "        all_results.extend(result_list)  # Collect all results\n",
    "    if best_model is not None: #check for the best model\n",
    "        best_models[(item_id, provinsi_id)] = best_model  # Store best models\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(results_df.sort_values(by='rmse').head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Prediction on Test Data ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_sarima = test_df.copy()\n",
    "test_df_sarima = test_df_sarima.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = []\n",
    "\n",
    "for item_id in test_df_sarima['item'].unique():\n",
    "    for provinsi_id in test_df_sarima['provinsi'].unique():\n",
    "        train_subset = train_df_sarima[(train_df_sarima.index.get_level_values('item') == item_id) & (train_df_sarima.index.get_level_values('provinsi') == provinsi_id)]\n",
    "        test_subset = test_df_sarima[(test_df_sarima['item'] == item_id) & (test_df_sarima['provinsi'] == provinsi_id)]\n",
    "\n",
    "        #reindex test data:\n",
    "        test_subset = test_subset.reindex(pd.date_range(start=test_subset.index.min(), end=test_subset.index.max(), freq='D'))\n",
    "        test_subset['item'] = item_id\n",
    "        test_subset['provinsi'] = provinsi_id\n",
    "\n",
    "\n",
    "        # Check if we have a best model for this (item, provinsi)\n",
    "        if (item_id, provinsi_id) not in best_models:\n",
    "            # print(f\"Skipping item={item_id}, provinsi={provinsi_id} due to no best model.\") # Debugging\n",
    "            final_predictions.extend([np.nan] * len(test_subset))  # Fill with NaN if no model\n",
    "            continue\n",
    "\n",
    "        # Fit the final model on the *entire* training subset (including differencing!)\n",
    "        final_train_diff = train_subset['price_diff'].dropna()  # Drop NaNs *before* training\n",
    "        # Ensure we have consistent frequency\n",
    "        #final_train_diff = final_train_diff.asfreq('D')  # No Longer Needed\n",
    "        if final_train_diff.index.inferred_freq != 'D':\n",
    "           print(f\"Skipping item={item_id}, provinsi={provinsi_id}: Could not set freq for final model\") # Debugging\n",
    "           final_predictions.extend([np.nan] * len(test_subset)) #keep length consistent\n",
    "           continue\n",
    "        \n",
    "        if len(final_train_diff) < 2:\n",
    "          final_predictions.extend([np.nan] * len(test_subset))\n",
    "          continue\n",
    "\n",
    "\n",
    "        final_model = SARIMAX(final_train_diff, order=best_models[(item_id, provinsi_id)].model.order,\n",
    "                              seasonal_order=best_models[(item_id, provinsi_id)].model.seasonal_order,\n",
    "                              enforce_stationarity=False, enforce_invertibility=False)\n",
    "        final_model_fit = final_model.fit(disp=False, maxiter=1000)\n",
    "\n",
    "        # Forecast on the *differenced* data.\n",
    "        # Use get_prediction and .predicted_mean for efficiency\n",
    "        test_predictions_diff = final_model_fit.get_prediction(start=len(final_train_diff), end=len(final_train_diff) + len(test_subset) - 1).predicted_mean\n",
    "\n",
    "        # Inverse difference the predictions, using the *last* value from the *original* training data\n",
    "        last_train_value = train_subset['price'].dropna().iloc[-1]   # Get *last* original value\n",
    "\n",
    "        test_predictions = []\n",
    "        for i in range(len(test_predictions_diff)):\n",
    "            if i == 0:\n",
    "              pred_value = inverse_difference(last_train_value, test_predictions_diff.iloc[i])  # First prediction\n",
    "            else:\n",
    "              pred_value = inverse_difference(test_predictions[i-1], test_predictions_diff.iloc[i])  # Subsequent predictions\n",
    "            test_predictions.append(pred_value)\n",
    "\n",
    "        final_predictions.extend(test_predictions)\n",
    "\n",
    "\n",
    "test_df_sarima['price'] = final_predictions  # Add predictions to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse Log Transform\n",
    "test_df_sarima['price'] = np.expm1(test_df_sarima['price'])  # INVERSE TRANSFORM\n",
    "submission_sarima = test_df_sarima[['id', 'price']]\n",
    "submission_sarima.to_csv('submission_sarima.csv', index=False)\n",
    "print(\"File submission_sarima.csv telah dibuat.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
